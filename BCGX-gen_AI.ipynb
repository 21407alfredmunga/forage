{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac85be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Downloading 10-Ks for Microsoft\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000789019/000095017024087843/0000950170-24-087843-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000095017024087843/msft-20240630.htm\n",
      "‚úÖ Saved: ./10K_Filings/Microsoft/Microsoft_10K_2024-07-30_msft-20240630.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000789019/000095017023035122/0000950170-23-035122-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000095017023035122/msft-20230630.htm\n",
      "‚úÖ Saved: ./10K_Filings/Microsoft/Microsoft_10K_2023-07-27_msft-20230630.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000789019/000156459022026876/0001564590-22-026876-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\n",
      "‚úÖ Saved: ./10K_Filings/Microsoft/Microsoft_10K_2022-07-28_msft-10k_20220630.htm\n",
      "\n",
      "üì• Downloading 10-Ks for Apple\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000320193/000032019324000123/0000320193-24-000123-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\n",
      "‚úÖ Saved: ./10K_Filings/Apple/Apple_10K_2024-11-01_aapl-20240928.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000320193/000032019323000106/0000320193-23-000106-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019323000106/aapl-20230930.htm\n",
      "‚úÖ Saved: ./10K_Filings/Apple/Apple_10K_2023-11-03_aapl-20230930.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000320193/000032019322000108/0000320193-22-000108-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\n",
      "‚úÖ Saved: ./10K_Filings/Apple/Apple_10K_2022-10-28_aapl-20220924.htm\n",
      "\n",
      "üì• Downloading 10-Ks for Tesla\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0001318605/000162828025003063/0001628280-25-003063-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/1318605/000162828025003063/tsla-20241231.htm\n",
      "‚úÖ Saved: ./10K_Filings/Tesla/Tesla_10K_2025-01-30_tsla-20241231.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0001318605/000162828024002390/0001628280-24-002390-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\n",
      "‚úÖ Saved: ./10K_Filings/Tesla/Tesla_10K_2024-01-29_tsla-20231231.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0001318605/000095017023001409/0000950170-23-001409-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm\n",
      "‚úÖ Saved: ./10K_Filings/Tesla/Tesla_10K_2023-01-31_tsla-20221231.htm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# SEC headers ‚Äì you must include a valid user-agent\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Your Name your.email@example.com\"  # ‚ö†Ô∏è Change this to your information\n",
    "}\n",
    "\n",
    "# Company CIKs (Central Index Key)\n",
    "companies = {\n",
    "    \"Microsoft\": \"0000789019\",\n",
    "    \"Apple\": \"0000320193\",\n",
    "    \"Tesla\": \"0001318605\"\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://data.sec.gov\"\n",
    "SEARCH_URL = \"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "\n",
    "def get_10k_urls(cik, count=3):\n",
    "    \"\"\"Get the URLs for the most recent 10-K filings for a company.\n",
    "    \n",
    "    Args:\n",
    "        cik (str): Company's Central Index Key\n",
    "        count (int): Number of 10-K filings to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        list: List of filing information (accession number, filing date)\n",
    "    \"\"\"\n",
    "    cik_padded = cik.zfill(10)\n",
    "    url = SEARCH_URL.format(cik=cik_padded)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS)\n",
    "        res.raise_for_status()\n",
    "        filings = res.json()['filings']['recent']\n",
    "        \n",
    "        filing_info = []\n",
    "        for i, form in enumerate(filings['form']):\n",
    "            if form == \"10-K\":\n",
    "                acc_num = filings['accessionNumber'][i]\n",
    "                filing_date = filings['filingDate'][i]\n",
    "                filing_info.append((acc_num, filing_date))\n",
    "                if len(filing_info) == count:\n",
    "                    break\n",
    "        return filing_info\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching filing URLs: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_10k_documents(company_name, cik):\n",
    "    \"\"\"Download 10-K filings for a company.\n",
    "    \n",
    "    Args:\n",
    "        company_name (str): Name of the company\n",
    "        cik (str): Company's Central Index Key\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• Downloading 10-Ks for {company_name}\")\n",
    "    filing_info = get_10k_urls(cik)\n",
    "    folder = f\"./10K_Filings/{company_name}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    if not filing_info:\n",
    "        print(f\"‚ö†Ô∏è No 10-K filings found for {company_name}\")\n",
    "        return\n",
    "\n",
    "    for acc_num, filing_date in filing_info:\n",
    "        try:\n",
    "            # Format accession number for URL (remove dashes for directory, keep dashes for filename)\n",
    "            acc_num_no_dashes = acc_num.replace('-', '')\n",
    "            \n",
    "            # First get the index page that lists all the documents in the filing\n",
    "            index_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{acc_num_no_dashes}/{acc_num}-index.htm\"\n",
    "            \n",
    "            print(f\"Accessing index: {index_url}\")\n",
    "            res = requests.get(index_url, headers=HEADERS)\n",
    "            res.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML to find the actual 10-K document\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            table = soup.find('table', summary='Document Format Files')\n",
    "            \n",
    "            if not table:\n",
    "                print(f\"‚ö†Ô∏è Could not find document table for {company_name}, filing date {filing_date}\")\n",
    "                continue\n",
    "                \n",
    "            # Look for the primary 10-K document\n",
    "            found_file = False\n",
    "            for row in table.find_all('tr'):\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 3:\n",
    "                    # Check if this is the 10-K document (either in type or description)\n",
    "                    doc_type = cells[0].get_text().strip().lower()\n",
    "                    description = cells[1].get_text().strip().lower()\n",
    "                    \n",
    "                    if '10-k' in doc_type or '10-k' in description:\n",
    "                        # Get the document link\n",
    "                        doc_link = cells[2].find('a')\n",
    "                        if doc_link and doc_link.has_attr('href'):\n",
    "                            doc_href = doc_link['href']\n",
    "                            \n",
    "                            # Convert relative URL to absolute URL\n",
    "                            if doc_href.startswith('/'):\n",
    "                                doc_url = f\"https://www.sec.gov{doc_href}\"\n",
    "                            else:\n",
    "                                doc_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{acc_num_no_dashes}/{doc_href}\"\n",
    "                            \n",
    "                            print(f\"Downloading document: {doc_url}\")\n",
    "                            doc = requests.get(doc_url, headers=HEADERS)\n",
    "                            doc.raise_for_status()\n",
    "                            \n",
    "                            # Extract filename for saving\n",
    "                            filename = os.path.basename(doc_href)\n",
    "                            file_path = os.path.join(folder, f\"{company_name}_10K_{filing_date}_{filename}\")\n",
    "                            \n",
    "                            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                                f.write(doc.text)\n",
    "                                \n",
    "                            print(f\"‚úÖ Saved: {file_path}\")\n",
    "                            found_file = True\n",
    "                            break\n",
    "            \n",
    "            if not found_file:\n",
    "                print(f\"‚ö†Ô∏è No suitable 10-K document found for {company_name} on {filing_date}\")\n",
    "                \n",
    "            # Be nice to the SEC server\n",
    "            time.sleep(1)\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error downloading filing: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing filing: {str(e)}\")\n",
    "\n",
    "# Download for each company\n",
    "for name, cik in companies.items():\n",
    "    download_10k_documents(name, cik)\n",
    "    time.sleep(2)  # Add delay between companies to avoid rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63eccf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6560e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_10k_to_csv():\n",
    "    \"\"\"\n",
    "    Process downloaded 10-K HTML files into CSV format.\n",
    "    Extracts key sections and organizes them into a structured CSV file.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Converting 10-K files to CSV format...\")\n",
    "    \n",
    "    # Create a list to store all data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process each company's filings\n",
    "    for company_name in companies.keys():\n",
    "        folder = f\"./10K_Filings/{company_name}\"\n",
    "        try:\n",
    "            files = [f for f in os.listdir(folder) if f.endswith('.htm')]\n",
    "            \n",
    "            for file in files:\n",
    "                file_path = os.path.join(folder, file)\n",
    "                \n",
    "                # Extract filing date from filename\n",
    "                match = re.search(r'10K_(\\d{4}-\\d{2}-\\d{2})', file)\n",
    "                filing_date = match.group(1) if match else \"Unknown\"\n",
    "                \n",
    "                # Read HTML content\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    html_content = f.read()\n",
    "                \n",
    "                # Parse HTML\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                \n",
    "                # Extract text content (removing scripts, styles)\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract()\n",
    "                text = soup.get_text()\n",
    "                \n",
    "                # Clean text (remove excessive whitespace)\n",
    "                text = re.sub(r'\\n+', '\\n', text)\n",
    "                text = re.sub(r' +', ' ', text)\n",
    "                \n",
    "                # Extract key sections (simplified approach)\n",
    "                sections = {\n",
    "                    'Risk Factors': extract_section(text, ['Item 1A.', 'ITEM 1A.', 'Risk Factors'], \n",
    "                                                  ['Item 1B', 'ITEM 1B', 'Item 2']),\n",
    "                    'Management Discussion': extract_section(text, ['Item 7.', 'ITEM 7.', \"Management's Discussion\"], \n",
    "                                                           ['Item 7A', 'ITEM 7A', 'Item 8']),\n",
    "                    'Business Description': extract_section(text, ['Item 1.', 'ITEM 1.', 'Business'], \n",
    "                                                          ['Item 1A', 'ITEM 1A']),\n",
    "                }\n",
    "                \n",
    "                # Truncate long sections to avoid CSV issues\n",
    "                max_length = 32000\n",
    "                for section, content in sections.items():\n",
    "                    if content and len(content) > max_length:\n",
    "                        sections[section] = content[:max_length] + \"... [truncated]\"\n",
    "                \n",
    "                # Add to data collection\n",
    "                all_data.append({\n",
    "                    'Company': company_name,\n",
    "                    'Filing Date': filing_date,\n",
    "                    'Year': filing_date.split('-')[0],\n",
    "                    'Risk Factors': sections['Risk Factors'],\n",
    "                    'Management Discussion': sections['Management Discussion'],\n",
    "                    'Business Description': sections['Business Description'],\n",
    "                    'File': file\n",
    "                })\n",
    "                \n",
    "                print(f\"‚úÖ Processed: {file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing company {company_name}: {str(e)}\")\n",
    "    \n",
    "    if all_data:\n",
    "        # Create DataFrame and save to CSV\n",
    "        df = pd.DataFrame(all_data)\n",
    "        csv_path = './10K_Analysis.csv'\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ CSV file created: {csv_path}\")\n",
    "        \n",
    "        # Also save a summary CSV with just metadata\n",
    "        summary_df = df[['Company', 'Filing Date', 'Year', 'File']]\n",
    "        summary_df.to_csv('./10K_Summary.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Summary CSV file created: ./10K_Summary.csv\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data to convert to CSV.\")\n",
    "\n",
    "def extract_section(text, start_markers, end_markers):\n",
    "    \"\"\"\n",
    "    Extract a section from text based on start and end markers.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full text to search\n",
    "        start_markers (list): List of possible section start markers\n",
    "        end_markers (list): List of possible section end markers\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted section or empty string if not found\n",
    "    \"\"\"\n",
    "    # Find the earliest occurrence of any start marker\n",
    "    start_pos = len(text)\n",
    "    for marker in start_markers:\n",
    "        pos = text.find(marker)\n",
    "        if pos != -1 and pos < start_pos:\n",
    "            start_pos = pos\n",
    "    \n",
    "    if start_pos == len(text):\n",
    "        return \"\"  # No start marker found\n",
    "    \n",
    "    # Find the earliest occurrence of any end marker after start_pos\n",
    "    end_pos = len(text)\n",
    "    for marker in end_markers:\n",
    "        pos = text.find(marker, start_pos + 1)\n",
    "        if pos != -1 and pos < end_pos:\n",
    "            end_pos = pos\n",
    "    \n",
    "    # Extract the section\n",
    "    section = text[start_pos:end_pos].strip()\n",
    "    return section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80218390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Downloading 10-Ks for Microsoft\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000789019/000095017024087843/0000950170-24-087843-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000095017024087843/msft-20240630.htm\n",
      "‚úÖ Saved: ./10K_Filings/Microsoft/Microsoft_10K_2024-07-30_msft-20240630.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000789019/000095017023035122/0000950170-23-035122-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000095017023035122/msft-20230630.htm\n",
      "‚úÖ Saved: ./10K_Filings/Microsoft/Microsoft_10K_2023-07-27_msft-20230630.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000789019/000156459022026876/0001564590-22-026876-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\n",
      "‚úÖ Saved: ./10K_Filings/Microsoft/Microsoft_10K_2022-07-28_msft-10k_20220630.htm\n",
      "\n",
      "üì• Downloading 10-Ks for Apple\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000320193/000032019324000123/0000320193-24-000123-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\n",
      "‚úÖ Saved: ./10K_Filings/Apple/Apple_10K_2024-11-01_aapl-20240928.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000320193/000032019323000106/0000320193-23-000106-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019323000106/aapl-20230930.htm\n",
      "‚úÖ Saved: ./10K_Filings/Apple/Apple_10K_2023-11-03_aapl-20230930.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0000320193/000032019322000108/0000320193-22-000108-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\n",
      "‚úÖ Saved: ./10K_Filings/Apple/Apple_10K_2022-10-28_aapl-20220924.htm\n",
      "\n",
      "üì• Downloading 10-Ks for Tesla\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0001318605/000162828025003063/0001628280-25-003063-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/1318605/000162828025003063/tsla-20241231.htm\n",
      "‚úÖ Saved: ./10K_Filings/Tesla/Tesla_10K_2025-01-30_tsla-20241231.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0001318605/000162828024002390/0001628280-24-002390-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\n",
      "‚úÖ Saved: ./10K_Filings/Tesla/Tesla_10K_2024-01-29_tsla-20231231.htm\n",
      "Accessing index: https://www.sec.gov/Archives/edgar/data/0001318605/000095017023001409/0000950170-23-001409-index.htm\n",
      "Downloading document: https://www.sec.gov/ix?doc=/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm\n",
      "‚úÖ Saved: ./10K_Filings/Tesla/Tesla_10K_2023-01-31_tsla-20221231.htm\n",
      "\n",
      "üìä Converting 10-K files to CSV format...\n",
      "‚úÖ Processed: Microsoft_10K_2024-07-30_msft-20240630.htm\n",
      "‚úÖ Processed: Microsoft_10K_2022-07-28_msft-10k_20220630.htm\n",
      "‚úÖ Processed: Microsoft_10K_2023-07-27_msft-20230630.htm\n",
      "‚úÖ Processed: Apple_10K_2022-10-28_aapl-20220924.htm\n",
      "‚úÖ Processed: Apple_10K_2023-11-03_aapl-20230930.htm\n",
      "‚úÖ Processed: Apple_10K_2024-11-01_aapl-20240928.htm\n",
      "‚úÖ Processed: Tesla_10K_2024-01-29_tsla-20231231.htm\n",
      "‚úÖ Processed: Tesla_10K_2023-01-31_tsla-20221231.htm\n",
      "‚úÖ Processed: Tesla_10K_2025-01-30_tsla-20241231.htm\n",
      "‚úÖ CSV file created: ./10K_Analysis.csv\n",
      "‚úÖ Summary CSV file created: ./10K_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Add this at the end of your notebook\n",
    "\n",
    "# Download for each company\n",
    "for name, cik in companies.items():\n",
    "    download_10k_documents(name, cik)\n",
    "    time.sleep(2)  # Add delay between companies to avoid rate limiting\n",
    "\n",
    "# After downloading, process the files to CSV\n",
    "process_10k_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6cce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 rows of 10-K data\n",
      "Companies: ['Microsoft' 'Apple' 'Tesla']\n",
      "Years: [np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "\n",
      "Preview of the data:\n",
      "     Company Filing Date  Year  Risk Factors  Management Discussion  \\\n",
      "0  Microsoft  2024-07-30  2024           NaN                    NaN   \n",
      "1  Microsoft  2022-07-28  2022           NaN                    NaN   \n",
      "2  Microsoft  2023-07-27  2023           NaN                    NaN   \n",
      "3      Apple  2022-10-28  2022           NaN                    NaN   \n",
      "4      Apple  2023-11-03  2023           NaN                    NaN   \n",
      "\n",
      "   Business Description                                            File  \n",
      "0                   NaN      Microsoft_10K_2024-07-30_msft-20240630.htm  \n",
      "1                   NaN  Microsoft_10K_2022-07-28_msft-10k_20220630.htm  \n",
      "2                   NaN      Microsoft_10K_2023-07-27_msft-20230630.htm  \n",
      "3                   NaN          Apple_10K_2022-10-28_aapl-20220924.htm  \n",
      "4                   NaN          Apple_10K_2023-11-03_aapl-20230930.htm  \n",
      "\n",
      "Missing data count per column:\n",
      "Company                  0\n",
      "Filing Date              0\n",
      "Year                     0\n",
      "Risk Factors             9\n",
      "Management Discussion    9\n",
      "Business Description     9\n",
      "File                     0\n",
      "dtype: int64\n",
      "\n",
      "Microsoft filings: 3\n",
      "Filings from 2023: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Read the existing CSV file\n",
    "df = pd.read_csv('10K_Analysis.csv')\n",
    "\n",
    "# Display the basic info\n",
    "print(f\"Loaded {len(df)} rows of 10-K data\")\n",
    "print(f\"Companies: {df['Company'].unique()}\")\n",
    "print(f\"Years: {sorted(df['Year'].unique())}\")\n",
    "\n",
    "# Preview the DataFrame\n",
    "print(\"\\nPreview of the data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing data\n",
    "print(\"\\nMissing data count per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# You can access specific company data\n",
    "microsoft_data = df[df['Company'] == 'Microsoft']\n",
    "print(f\"\\nMicrosoft filings: {len(microsoft_data)}\")\n",
    "\n",
    "# You can filter by year\n",
    "data_2023 = df[df['Year'] == 2023]\n",
    "print(f\"Filings from 2023: {len(data_2023)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac2fc0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updating sections in the DataFrame...\n",
      "‚úÖ Updated sections for Microsoft_10K_2024-07-30_msft-20240630.htm\n",
      "‚úÖ Updated sections for Microsoft_10K_2022-07-28_msft-10k_20220630.htm\n",
      "‚úÖ Updated sections for Microsoft_10K_2023-07-27_msft-20230630.htm\n",
      "‚úÖ Updated sections for Apple_10K_2022-10-28_aapl-20220924.htm\n",
      "‚úÖ Updated sections for Apple_10K_2023-11-03_aapl-20230930.htm\n",
      "‚úÖ Updated sections for Apple_10K_2024-11-01_aapl-20240928.htm\n",
      "‚úÖ Updated sections for Tesla_10K_2024-01-29_tsla-20231231.htm\n",
      "‚úÖ Updated sections for Tesla_10K_2023-01-31_tsla-20221231.htm\n",
      "‚úÖ Updated sections for Tesla_10K_2025-01-30_tsla-20241231.htm\n",
      "‚úÖ Saved updated DataFrame to '10K_Analysis_updated.csv'\n",
      "\n",
      "Extracted section counts:\n",
      "Risk Factors: 9\n",
      "Management Discussion: 9\n",
      "Business Description: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25651/3158672488.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'Risk Factors'] = (risk_factors[:max_length] + '... [truncated]'\n",
      "/tmp/ipykernel_25651/3158672488.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'Management Discussion'] = (mgmt_discussion[:max_length] + '... [truncated]'\n",
      "/tmp/ipykernel_25651/3158672488.py:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'Business Description'] = (business_desc[:max_length] + '... [truncated]'\n"
     ]
    }
   ],
   "source": [
    "def update_sections_in_dataframe():\n",
    "    \"\"\"Update the sections in the DataFrame by reprocessing the HTML files.\"\"\"\n",
    "    print(\"\\nUpdating sections in the DataFrame...\")\n",
    "    \n",
    "    # For each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        company = row['Company']\n",
    "        filename = row['File']\n",
    "        file_path = os.path.join('./10K_Filings', company, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Read HTML content\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "                \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                \n",
    "            # Extract text content (removing scripts, styles)\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            text = soup.get_text()\n",
    "                \n",
    "            # Clean text (remove excessive whitespace)\n",
    "            text = re.sub(r'\\n+', '\\n', text)\n",
    "            text = re.sub(r' +', ' ', text)\n",
    "                \n",
    "            # Extract key sections with improved patterns\n",
    "            risk_factors = extract_section(\n",
    "                text, \n",
    "                ['Item 1A', 'ITEM 1A', 'Risk Factors'], \n",
    "                ['Item 1B', 'ITEM 1B', 'Item 2', 'ITEM 2']\n",
    "            )\n",
    "                \n",
    "            mgmt_discussion = extract_section(\n",
    "                text, \n",
    "                ['Item 7', 'ITEM 7', \"Management's Discussion\"], \n",
    "                ['Item 7A', 'ITEM 7A', 'Item 8', 'ITEM 8']\n",
    "            )\n",
    "                \n",
    "            business_desc = extract_section(\n",
    "                text, \n",
    "                ['Item 1', 'ITEM 1', 'Business'], \n",
    "                ['Item 1A', 'ITEM 1A']\n",
    "            )\n",
    "                \n",
    "            # Update the DataFrame with extracted content\n",
    "            # Truncate if necessary\n",
    "            max_length = 32000\n",
    "            df.at[index, 'Risk Factors'] = (risk_factors[:max_length] + '... [truncated]' \n",
    "                                           if len(risk_factors) > max_length else risk_factors)\n",
    "            df.at[index, 'Management Discussion'] = (mgmt_discussion[:max_length] + '... [truncated]' \n",
    "                                                  if len(mgmt_discussion) > max_length else mgmt_discussion)\n",
    "            df.at[index, 'Business Description'] = (business_desc[:max_length] + '... [truncated]' \n",
    "                                                 if len(business_desc) > max_length else business_desc)\n",
    "                \n",
    "            print(f\"‚úÖ Updated sections for {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    df.to_csv('10K_Analysis_updated.csv', index=False)\n",
    "    print(\"‚úÖ Saved updated DataFrame to '10K_Analysis_updated.csv'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_section(text, start_markers, end_markers):\n",
    "    \"\"\"\n",
    "    Extract a section from text based on start and end markers.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full text to search\n",
    "        start_markers (list): List of possible section start markers\n",
    "        end_markers (list): List of possible section end markers\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted section or empty string if not found\n",
    "    \"\"\"\n",
    "    # Find the earliest occurrence of any start marker\n",
    "    start_pos = len(text)\n",
    "    for marker in start_markers:\n",
    "        pos = text.find(marker)\n",
    "        if pos != -1 and pos < start_pos:\n",
    "            start_pos = pos\n",
    "    \n",
    "    if start_pos == len(text):\n",
    "        return \"\"  # No start marker found\n",
    "    \n",
    "    # Find the earliest occurrence of any end marker after start_pos\n",
    "    end_pos = len(text)\n",
    "    for marker in end_markers:\n",
    "        pos = text.find(marker, start_pos + 1)\n",
    "        if pos != -1 and pos < end_pos:\n",
    "            end_pos = pos\n",
    "    \n",
    "    # Extract the section\n",
    "    section = text[start_pos:end_pos].strip()\n",
    "    return section\n",
    "\n",
    "# Run the function to update the sections\n",
    "updated_df = update_sections_in_dataframe()\n",
    "\n",
    "# Check how many sections were successfully extracted\n",
    "print(\"\\nExtracted section counts:\")\n",
    "print(f\"Risk Factors: {updated_df['Risk Factors'].notna().sum()}\")\n",
    "print(f\"Management Discussion: {updated_df['Management Discussion'].notna().sum()}\")\n",
    "print(f\"Business Description: {updated_df['Business Description'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a284913",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10K_Filings/Apple/Apple_10K_2022-10-28_aapl-20220924.htm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/Desktop/forage/datascience_env/lib/python3.10/site-packages/bs4/__init__.py:440\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;241m=\u001b[39m parse_only\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m     markup \u001b[38;5;241m=\u001b[39m \u001b[43mmarkup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncoming markup is of an invalid type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarkup\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. Markup must be a string, a bytestring, or an open filehandle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "soup = BeautifulSoup(file, \"html.parser\")\n",
    "# Load the file\n",
    "with open(\"10K_Filings/Apple/Apple_10K_2022-10-28_aapl-20220924.htm\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"lxml\")\n",
    "\n",
    "# Extract all tables\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# Convert tables to DataFrames\n",
    "dfs = [pd.read_html(str(table))[0] for table in tables]\n",
    "\n",
    "# Example: print first table\n",
    "if dfs:\n",
    "    print(dfs[0].head())\n",
    "else:\n",
    "    print(\"No tables found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d94a9ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;32m----> 3\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the file\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10K_Filings/Apple/Apple_10K_2022-10-28_aapl-20220924.htm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\n",
      "File \u001b[0;32m~/Desktop/forage/datascience_env/lib/python3.10/site-packages/bs4/__init__.py:440\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;241m=\u001b[39m parse_only\n",
      "\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n",
      "\u001b[0;32m--> 440\u001b[0m     markup \u001b[38;5;241m=\u001b[39m \u001b[43mmarkup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n",
      "\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncoming markup is of an invalid type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarkup\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. Markup must be a string, a bytestring, or an open filehandle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    444\u001b[0m     )\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "soup = BeautifulSoup(file, \"html.parser\")\n",
    "# Load the file\n",
    "with open(\"10K_Filings/Apple/Apple_10K_2022-10-28_aapl-20220924.htm\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"lxml\")\n",
    "\n",
    "# Extract all tables\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# Convert tables to DataFrames\n",
    "dfs = [pd.read_html(str(table))[0] for table in tables]\n",
    "\n",
    "# Example: print first table\n",
    "if dfs:\n",
    "    print(dfs[0].head())\n",
    "else:\n",
    "    print(\"No tables found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a645339",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(file, \"html.parser\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

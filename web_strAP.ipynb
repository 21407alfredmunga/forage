{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a87bde",
   "metadata": {},
   "source": [
    "# Web Scraping Naivas Online for Mattress Prices\n",
    "\n",
    "This notebook demonstrates how to scrape mattress price information from the Naivas online store (https://naivas.online/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c349062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./datascience_env/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./datascience_env/lib/python3.10/site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in ./datascience_env/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./datascience_env/lib/python3.10/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./datascience_env/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./datascience_env/lib/python3.10/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./datascience_env/lib/python3.10/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./datascience_env/lib/python3.10/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./datascience_env/lib/python3.10/site-packages (from beautifulsoup4) (4.14.0)\n",
      "Requirement already satisfied: pandas in ./datascience_env/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./datascience_env/lib/python3.10/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./datascience_env/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./datascience_env/lib/python3.10/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./datascience_env/lib/python3.10/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./datascience_env/lib/python3.10/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./datascience_env/lib/python3.10/site-packages (from beautifulsoup4) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./datascience_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./datascience_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./datascience_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e643d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapingbee\n",
      "  Downloading scrapingbee-2.0.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Downloading scrapingbee-2.0.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: requests in ./datascience_env/lib/python3.10/site-packages (from scrapingbee) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (2025.4.26)\n",
      "Requirement already satisfied: requests in ./datascience_env/lib/python3.10/site-packages (from scrapingbee) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./datascience_env/lib/python3.10/site-packages (from requests->scrapingbee) (2025.4.26)\n",
      "Downloading scrapingbee-2.0.1-py3-none-any.whl (5.2 kB)\n",
      "Downloading scrapingbee-2.0.1-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: scrapingbee\n",
      "Installing collected packages: scrapingbee\n",
      "Successfully installed scrapingbee-2.0.1\n",
      "Successfully installed scrapingbee-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Install ScrapingBee client library\n",
    "!pip install scrapingbee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7589edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base URL and search URL\n",
    "base_url = 'https://naivas.online/'\n",
    "search_url = urljoin(base_url, 'search?q=mattress')\n",
    "\n",
    "# Set up enhanced headers to better mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://naivas.online/',\n",
    "    'Sec-Ch-Ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\"',\n",
    "    'Sec-Ch-Ua-Mobile': '?0',\n",
    "    'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "}\n",
    "\n",
    "# Create a persistent session\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "# Print a note about 403 errors\n",
    "print(\"Note: If you encounter 403 Forbidden errors, the website may be blocking web scraping attempts.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e2212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScrapingBee implementation\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# You need to sign up on ScrapingBee.com to get your API key\n",
    "SCRAPING_BEE_API_KEY = \"RGE1GF6765MPKI50NKYTUXKKIL5LGXVE1YSLFXHEAIVL9PRCQ67EVU92KGQJ6QP1D4YJB4H91NBKV58K\"  # Replace with your actual ScrapingBee API key\n",
    "\n",
    "def get_soup_with_scrapingbee(url):\n",
    "    \"\"\"\n",
    "    Use ScrapingBee API to bypass anti-scraping protections\n",
    "    \"\"\"\n",
    "    print(f\"Sending request to {url} via ScrapingBee\")\n",
    "    \n",
    "    # Set up ScrapingBee parameters\n",
    "    params = {\n",
    "        \"api_key\": SCRAPING_BEE_API_KEY,\n",
    "        \"url\": url,\n",
    "        \"premium_proxy\": \"true\",  # Use premium proxies to avoid being blocked\n",
    "        \"country_code\": \"ke\",     # Kenya, as we're accessing a Kenyan website\n",
    "        \"render_js\": \"true\",      # Render JavaScript to ensure all content loads\n",
    "        \"js_scroll\": \"true\",      # Scroll the page to load lazy-loaded elements\n",
    "    }\n",
    "    \n",
    "    # Construct the API endpoint with parameters\n",
    "    api_url = f\"https://app.scrapingbee.com/api/v1/?{urlencode(params)}\"\n",
    "    \n",
    "    try:\n",
    "        # Send request to ScrapingBee API\n",
    "        response = requests.get(api_url, timeout=60)  # Longer timeout for rendered pages\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return soup\n",
    "        else:\n",
    "            # Print error details from ScrapingBee\n",
    "            print(f\"ScrapingBee error - Status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error using ScrapingBee: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ccd48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access the search page directly...\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "Trying alternative access methods...\n",
      "Method 1: Visiting homepage first...\n",
      "Successfully accessed homepage. Now trying search page...\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "Trying alternative access methods...\n",
      "Method 1: Visiting homepage first...\n",
      "Successfully accessed homepage. Now trying search page...\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "All attempts failed. The website might be actively blocking scraping attempts.\n",
      "Possible solutions:\n",
      "1. Use a proper web scraping service that rotates IP addresses\n",
      "2. Consider using the official API if available\n",
      "3. Look into using Selenium to simulate a real browser\n",
      "4. Respect the website's robots.txt and terms of service\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "All attempts failed. The website might be actively blocking scraping attempts.\n",
      "Possible solutions:\n",
      "1. Use a proper web scraping service that rotates IP addresses\n",
      "2. Consider using the official API if available\n",
      "3. Look into using Selenium to simulate a real browser\n",
      "4. Respect the website's robots.txt and terms of service\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = 'https://www.example.com'\n",
    "\n",
    "# Search URL (this should be the URL of the search results page)\n",
    "search_url = urljoin(base_url, 'search/results')\n",
    "\n",
    "# Headers to simulate a real browser (update with real user-agent if needed)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Start a session\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "# Function to get the webpage content with enhanced error handling\n",
    "def get_soup(url, retry_count=3, delay=2):\n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            # Add a delay to be respectful to the server and avoid detection\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            # Send the request using the persistent session\n",
    "            print(f\"Sending request to {url} (Attempt {attempt+1}/{retry_count})\")\n",
    "            response = session.get(url, timeout=10)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                return soup\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"Access Forbidden (403). The website may be blocking scraping attempts.\")\n",
    "                # Increase delay before next attempt\n",
    "                delay += 2\n",
    "            else:\n",
    "                print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "                \n",
    "            # If this wasn't the last attempt, wait before trying again\n",
    "            if attempt < retry_count - 1:\n",
    "                wait_time = delay * (attempt + 1)  # Progressive backoff\n",
    "                print(f\"Waiting {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                wait_time = delay * (attempt + 1)\n",
    "                print(f\"Waiting {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Try an alternative approach if direct access doesn't work\n",
    "def try_alternative_access_methods():\n",
    "    print(\"\\nTrying alternative access methods...\")\n",
    "    \n",
    "    # Method 1: Access the homepage first, then navigate to search\n",
    "    try:\n",
    "        print(\"Method 1: Visiting homepage first...\")\n",
    "        home_page = session.get(base_url, timeout=10)\n",
    "        if home_page.status_code == 200:\n",
    "            print(\"Successfully accessed homepage. Now trying search page...\")\n",
    "            time.sleep(3)  # Wait before next request\n",
    "            return get_soup(search_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "    \n",
    "    # Method 2: Try with a different search query\n",
    "    try:\n",
    "        print(\"\\nMethod 2: Trying with a different search term...\")\n",
    "        alt_search_url = urljoin(base_url, 'search?q=furniture')\n",
    "        return get_soup(alt_search_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Method 2 failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# First try the direct approach\n",
    "print(\"Attempting to access the search page directly...\")\n",
    "soup = get_soup(search_url)\n",
    "\n",
    "# If direct access fails, try alternatives\n",
    "if not soup:\n",
    "    soup = try_alternative_access_methods()\n",
    "\n",
    "if soup:\n",
    "    print(\"\\nSuccess! Successfully retrieved the webpage.\")\n",
    "else:\n",
    "    print(\"\\nAll attempts failed. The website might be actively blocking scraping attempts.\")\n",
    "    print(\"Possible solutions:\")\n",
    "    print(\"1. Use a proper web scraping service that rotates IP addresses\")\n",
    "    print(\"2. Consider using the official API if available\")\n",
    "    print(\"3. Look into using Selenium to simulate a real browser\")\n",
    "    print(\"4. Respect the website's robots.txt and terms of service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d0a632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1: Using ScrapingBee API...\n",
      "Sending request to https://www.example.com/search/results via ScrapingBee\n",
      "ScrapingBee error - Status code: 404\n",
      "Response: <!DOCTYPE html><html><head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "\n",
      "\n",
      "</body></html>\n",
      "\n",
      "ScrapingBee attempt failed. Trying alternative methods...\n",
      "\n",
      "Attempt 2: Using enhanced request headers...\n",
      "ScrapingBee error - Status code: 404\n",
      "Response: <!DOCTYPE html><html><head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "\n",
      "\n",
      "</body></html>\n",
      "\n",
      "ScrapingBee attempt failed. Trying alternative methods...\n",
      "\n",
      "Attempt 2: Using enhanced request headers...\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "Standard methods failed. Trying more aggressive approaches...\n",
      "\n",
      "Trying alternative access methods...\n",
      "Method 1: Visiting homepage first...\n",
      "Successfully accessed homepage. Now trying search page...\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "Standard methods failed. Trying more aggressive approaches...\n",
      "\n",
      "Trying alternative access methods...\n",
      "Method 1: Visiting homepage first...\n",
      "Successfully accessed homepage. Now trying search page...\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 1/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 2 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 2/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "Waiting 4 seconds before retrying...\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Sending request to https://www.example.com/search/results (Attempt 3/3)\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "❌ All attempts failed. Consider these options:\n",
      "1. Check that your ScrapingBee API key is valid\n",
      "2. Try using Selenium with browser automation\n",
      "3. Check if the website has an official API\n",
      "4. Consider if manual data collection is feasible for your needs\n",
      "Failed to fetch the page. Status code: 404\n",
      "\n",
      "❌ All attempts failed. Consider these options:\n",
      "1. Check that your ScrapingBee API key is valid\n",
      "2. Try using Selenium with browser automation\n",
      "3. Check if the website has an official API\n",
      "4. Consider if manual data collection is feasible for your needs\n"
     ]
    }
   ],
   "source": [
    "# ScrapingBee wrapper function with fallback to original methods\n",
    "def scrape_with_fallback(url, max_attempts=2):\n",
    "    \"\"\"\n",
    "    Try scraping with ScrapingBee first, then fall back to other methods if it fails\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempt 1: Using ScrapingBee API...\")\n",
    "    soup = get_soup_with_scrapingbee(url)\n",
    "    \n",
    "    if soup:\n",
    "        print(\"✅ Success using ScrapingBee!\")\n",
    "        return soup\n",
    "    \n",
    "    print(\"\\nScrapingBee attempt failed. Trying alternative methods...\")\n",
    "    \n",
    "    # Try original methods as fallbacks\n",
    "    print(\"\\nAttempt 2: Using enhanced request headers...\")\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    if soup:\n",
    "        print(\"✅ Success using enhanced headers!\")\n",
    "        return soup\n",
    "    \n",
    "    # If we still don't have the soup, try alternative methods\n",
    "    print(\"\\nStandard methods failed. Trying more aggressive approaches...\")\n",
    "    soup = try_alternative_access_methods()\n",
    "    \n",
    "    return soup\n",
    "\n",
    "# Try to scrape the mattress search page\n",
    "soup = scrape_with_fallback(search_url)\n",
    "\n",
    "if soup:\n",
    "    print(\"\\n✅ Successfully retrieved the webpage!\")\n",
    "    print(\"Page title:\", soup.title.text if soup.title else \"No title found\")\n",
    "    \n",
    "    # Print a few elements to verify we have the right content\n",
    "    print(\"\\nVerifying page content...\")\n",
    "    products = soup.find_all(['div', 'li'], class_=lambda x: x and ('product' in x.lower() or 'item' in x.lower()) if x else False)\n",
    "    print(f\"Found {len(products)} potential product elements\")\n",
    "    \n",
    "    # Show a sample of what we found\n",
    "    if products:\n",
    "        sample = products[0]\n",
    "        print(\"\\nSample product HTML structure:\")\n",
    "        print(sample.prettify()[:500] + \"...\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n❌ All attempts failed. Consider these options:\")\n",
    "    print(\"1. Check that your ScrapingBee API key is valid\")\n",
    "    print(\"2. Try using Selenium with browser automation\")\n",
    "    print(\"3. Check if the website has an official API\")\n",
    "    print(\"4. Consider if manual data collection is feasible for your needs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Using Selenium (uncomment and install selenium if needed)\n",
    "\n",
    "# !pip install selenium webdriver-manager\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import time\n",
    "\n",
    "# def get_page_with_selenium(url, headless=True):\n",
    "#     try:\n",
    "#         # Set up Chrome options\n",
    "#         chrome_options = Options()\n",
    "#         if headless:\n",
    "#             chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "#         chrome_options.add_argument(\"--no-sandbox\")\n",
    "#         chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "#         chrome_options.add_argument(\"--disable-gpu\")\n",
    "#         chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "        \n",
    "#         # Create a new Chrome webdriver\n",
    "#         driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), \n",
    "#                                  options=chrome_options)\n",
    "        \n",
    "#         # Navigate to the specified URL\n",
    "#         print(f\"Navigating to {url} with Selenium...\")\n",
    "#         driver.get(url)\n",
    "        \n",
    "#         # Wait for the page to load\n",
    "#         time.sleep(5)\n",
    "        \n",
    "#         # Get the page source\n",
    "#         page_source = driver.page_source\n",
    "        \n",
    "#         # Parse with BeautifulSoup\n",
    "#         soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "#         # Close the browser\n",
    "#         driver.quit()\n",
    "        \n",
    "#         return soup\n",
    "#     except Exception as e:\n",
    "#         print(f\"Selenium error: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Try with Selenium if the previous methods fail\n",
    "# if not soup:\n",
    "#     print(\"\\nAttempting to use Selenium for browser automation...\")\n",
    "#     soup = get_page_with_selenium(search_url)\n",
    "    \n",
    "#     if soup:\n",
    "#         print(\"Successfully retrieved the page using Selenium!\")\n",
    "#     else:\n",
    "#         print(\"Selenium approach failed as well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5653cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m products\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Extract product information\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m products \u001b[38;5;241m=\u001b[39m \u001b[43mextract_product_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[1;32m     57\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(products)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mextract_product_info\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m      3\u001b[0m products \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This will need to be adjusted based on the actual HTML structure of the website\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Let's try to find product containers\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m product_containers \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m product_containers:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Try alternative selectors if the first attempt didn't work\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     product_containers \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv.product, div.item, li.product, li.item\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# Function to extract product information\n",
    "def extract_product_info(soup):\n",
    "    products = []\n",
    "    \n",
    "    # This will need to be adjusted based on the actual HTML structure of the website\n",
    "    # Let's try to find product containers\n",
    "    product_containers = soup.find_all('div', class_=lambda x: x and ('product' in x.lower() or 'item' in x.lower()))\n",
    "    \n",
    "    if not product_containers:\n",
    "        # Try alternative selectors if the first attempt didn't work\n",
    "        product_containers = soup.select('div.product, div.item, li.product, li.item')\n",
    "    \n",
    "    if not product_containers:\n",
    "        # Just get all divs with id or class containing product\n",
    "        product_containers = soup.find_all(['div', 'li'], id=lambda x: x and 'product' in x.lower())\n",
    "    \n",
    "    print(f\"Found {len(product_containers)} potential product containers\")\n",
    "    \n",
    "    for container in product_containers:\n",
    "        try:\n",
    "            # Try to extract product name\n",
    "            name_tag = container.find(['h2', 'h3', 'h4', 'a'], class_=lambda x: x and 'title' in x.lower() if x else False)\n",
    "            if not name_tag:\n",
    "                name_tag = container.find(['h2', 'h3', 'h4', 'a'])\n",
    "            \n",
    "            name = name_tag.get_text().strip() if name_tag else \"No name found\"\n",
    "            \n",
    "            # Try to extract price\n",
    "            price_tag = container.find(['span', 'div'], class_=lambda x: x and 'price' in x.lower() if x else False)\n",
    "            price = price_tag.get_text().strip() if price_tag else \"No price found\"\n",
    "            \n",
    "            # Clean price (removing currency symbols and formatting)\n",
    "            cleaned_price = re.sub(r'[^\\d.]', '', price) if price != \"No price found\" else \"No price found\"\n",
    "            \n",
    "            # Try to extract link\n",
    "            link_tag = container.find('a', href=True)\n",
    "            link = urljoin(base_url, link_tag['href']) if link_tag else \"No link found\"\n",
    "            \n",
    "            # Add to products list\n",
    "            if 'mattress' in name.lower() or 'bed' in name.lower():\n",
    "                products.append({\n",
    "                    'Name': name,\n",
    "                    'Price': price,\n",
    "                    'Numeric_Price': cleaned_price,\n",
    "                    'Link': link\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting product info: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "# Extract product information\n",
    "products = extract_product_info(soup)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Display the first few rows\n",
    "if not df.empty:\n",
    "    print(f\"Found {len(df)} mattress products\")\n",
    "    df.head()\n",
    "else:\n",
    "    print(\"No products found. The webpage might have a different structure than expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle pagination and extract products from multiple pages\n",
    "def scrape_all_pages(base_search_url, max_pages=5):\n",
    "    all_products = []\n",
    "    current_page = 1\n",
    "    \n",
    "    while current_page <= max_pages:\n",
    "        page_url = f\"{base_search_url}&page={current_page}\" if current_page > 1 else base_search_url\n",
    "        print(f\"Scraping page {current_page}: {page_url}\")\n",
    "        \n",
    "        soup = get_soup(page_url)\n",
    "        if not soup:\n",
    "            print(f\"Failed to retrieve page {current_page}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        page_products = extract_product_info(soup)\n",
    "        if not page_products:\n",
    "            print(f\"No products found on page {current_page}. This might be the last page.\")\n",
    "            break\n",
    "            \n",
    "        all_products.extend(page_products)\n",
    "        \n",
    "        # Check if there's a next page by looking for pagination links\n",
    "        next_page_link = soup.find('a', text=lambda x: x and 'Next' in x or '→' in x or '>' in x)\n",
    "        if not next_page_link:\n",
    "            next_page_link = soup.find('a', class_=lambda x: x and 'next' in x.lower() if x else False)\n",
    "        \n",
    "        if not next_page_link:\n",
    "            print(\"No next page link found. This might be the last page.\")\n",
    "            break\n",
    "            \n",
    "        current_page += 1\n",
    "        \n",
    "    return all_products\n",
    "\n",
    "# Uncomment the following lines to scrape all pages\n",
    "# all_products = scrape_all_pages(search_url)\n",
    "# all_products_df = pd.DataFrame(all_products)\n",
    "# all_products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get detailed information for each product\n",
    "def get_product_details(product_url):\n",
    "    soup = get_soup(product_url)\n",
    "    if not soup:\n",
    "        return {}\n",
    "    \n",
    "    details = {}\n",
    "    \n",
    "    # Try to extract product details\n",
    "    # This will need to be adjusted based on the actual structure of the product page\n",
    "    try:\n",
    "        # Extract product title\n",
    "        title_tag = soup.find(['h1', 'h2'], class_=lambda x: x and 'title' in x.lower() if x else False)\n",
    "        if not title_tag:\n",
    "            title_tag = soup.find(['h1', 'h2'])\n",
    "        details['Title'] = title_tag.get_text().strip() if title_tag else \"No title found\"\n",
    "        \n",
    "        # Extract product description\n",
    "        desc_tag = soup.find(['div', 'p'], class_=lambda x: x and 'description' in x.lower() if x else False)\n",
    "        details['Description'] = desc_tag.get_text().strip() if desc_tag else \"No description found\"\n",
    "        \n",
    "        # Extract specs/features\n",
    "        specs = {}\n",
    "        spec_container = soup.find('table') or soup.find('div', class_=lambda x: x and ('spec' in x.lower() or 'detail' in x.lower() or 'feature' in x.lower()) if x else False)\n",
    "        \n",
    "        if spec_container:\n",
    "            rows = spec_container.find_all(['tr', 'li'])\n",
    "            for row in rows:\n",
    "                if row.name == 'tr':\n",
    "                    cols = row.find_all(['th', 'td'])\n",
    "                    if len(cols) >= 2:\n",
    "                        key = cols[0].get_text().strip()\n",
    "                        value = cols[1].get_text().strip()\n",
    "                        specs[key] = value\n",
    "                else:  # li\n",
    "                    text = row.get_text().strip()\n",
    "                    if ':' in text:\n",
    "                        key, value = text.split(':', 1)\n",
    "                        specs[key.strip()] = value.strip()\n",
    "                    else:\n",
    "                        specs[f\"Feature {len(specs) + 1}\"] = text\n",
    "                        \n",
    "        details['Specifications'] = specs\n",
    "        \n",
    "        # Extract images\n",
    "        img_tags = soup.find_all('img', src=True)\n",
    "        product_images = [urljoin(base_url, img['src']) for img in img_tags if 'product' in img.get('class', [''])[0].lower() if img.get('class') else False]\n",
    "        details['Images'] = product_images if product_images else []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting product details: {e}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "# Example usage to get detailed information for a single product\n",
    "# (Uncomment and run after getting the product links)\n",
    "\n",
    "# if not df.empty and 'Link' in df.columns:\n",
    "#     sample_product = df.iloc[0]\n",
    "#     print(f\"Getting details for: {sample_product['Name']}\")\n",
    "#     details = get_product_details(sample_product['Link'])\n",
    "#     print(\"Product Details:\")\n",
    "#     for key, value in details.items():\n",
    "#         print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis (uncomment and run after collecting data)\n",
    "\n",
    "# Convert price to numeric\n",
    "# df['Numeric_Price'] = pd.to_numeric(df['Numeric_Price'], errors='coerce')\n",
    "\n",
    "# # Basic statistics\n",
    "# if 'Numeric_Price' in df.columns:\n",
    "#     print(\"Price Statistics:\")\n",
    "#     print(df['Numeric_Price'].describe())\n",
    "    \n",
    "#     # Import visualization libraries\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import seaborn as sns\n",
    "    \n",
    "#     # Set style\n",
    "#     plt.style.use('seaborn')\n",
    "#     sns.set(font_scale=1.2)\n",
    "    \n",
    "#     # Price distribution\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.histplot(df['Numeric_Price'].dropna(), kde=True)\n",
    "#     plt.title('Distribution of Mattress Prices')\n",
    "#     plt.xlabel('Price (KSH)')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Boxplot to see outliers\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(y=df['Numeric_Price'].dropna())\n",
    "#     plt.title('Boxplot of Mattress Prices')\n",
    "#     plt.ylabel('Price (KSH)')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c69ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Save the data to a CSV file\n",
    "def save_to_csv(dataframe, filename='naivas_mattresses.csv'):\n",
    "    try:\n",
    "        dataframe.to_csv(filename, index=False)\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to CSV: {e}\")\n",
    "\n",
    "# Function to handle pagination and extract products from multiple pages\n",
    "def scrape_all_pages(base_search_url, max_pages=5):\n",
    "    all_products = []\n",
    "    current_page = 1\n",
    "    \n",
    "    while current_page <= max_pages:\n",
    "        page_url = f\"{base_search_url}&page={current_page}\" if current_page > 1 else base_search_url\n",
    "        print(f\"Scraping page {current_page}: {page_url}\")\n",
    "        \n",
    "        # Use ScrapingBee to get the page\n",
    "        soup = scrape_with_fallback(page_url)\n",
    "        if not soup:\n",
    "            print(f\"Failed to retrieve page {current_page}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        page_products = extract_product_info(soup)\n",
    "        if not page_products:\n",
    "            print(f\"No products found on page {current_page}. This might be the last page.\")\n",
    "            break\n",
    "            \n",
    "        all_products.extend(page_products)\n",
    "        print(f\"Added {len(page_products)} products from page {current_page}\")\n",
    "        \n",
    "        # Check if there's a next page by looking for pagination links\n",
    "        next_page_link = soup.find('a', text=lambda x: x and 'Next' in x or '→' in x or '>' in x)\n",
    "        if not next_page_link:\n",
    "            next_page_link = soup.find('a', class_=lambda x: x and 'next' in x.lower() if x else False)\n",
    "        \n",
    "        if not next_page_link:\n",
    "            print(\"No next page link found. This might be the last page.\")\n",
    "            break\n",
    "            \n",
    "        current_page += 1\n",
    "        \n",
    "        # Add a delay between pages to be respectful to ScrapingBee API\n",
    "        if current_page <= max_pages:\n",
    "            print(f\"Waiting 5 seconds before fetching next page...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    print(f\"\\nScraped a total of {len(all_products)} products from {current_page} pages\")\n",
    "    return all_products\n",
    "\n",
    "# Uncomment the following lines to scrape all pages\n",
    "# all_products = scrape_all_pages(search_url, max_pages=3)  # Start with a small number to test\n",
    "# all_products_df = pd.DataFrame(all_products)\n",
    "# if not all_products_df.empty:\n",
    "#     print(\"\\nSample of scraped products:\")\n",
    "#     display(all_products_df.head())\n",
    "# else:\n",
    "#     print(\"No products found across all pages.\")\n",
    "\n",
    "# Uncomment to save the data\n",
    "# if not df.empty:\n",
    "#     save_to_csv(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f01ca",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This notebook provides a framework for scraping mattress price information from Naivas Online. The code can be adjusted based on the actual HTML structure of the website.\n",
    "\n",
    "Next steps could include:\n",
    "\n",
    "1. Running a more comprehensive analysis on the collected data\n",
    "2. Creating visualizations to compare prices by brand or type\n",
    "3. Setting up automated scraping to track price changes over time\n",
    "4. Adding more error handling and robustness to the scraper\n",
    "\n",
    "**Note:** Always be sure to check the website's terms of service and robots.txt file to ensure that web scraping is allowed.\n",
    "\n",
    "# Function to get detailed information for each product\n",
    "def get_product_details(product_url):\n",
    "    # Use ScrapingBee for product pages too\n",
    "    soup = scrape_with_fallback(product_url)\n",
    "    if not soup:\n",
    "        return {}\n",
    "    \n",
    "    details = {}\n",
    "    \n",
    "    # Try to extract product details\n",
    "    # This will need to be adjusted based on the actual structure of the product page\n",
    "    try:\n",
    "        # Extract product title\n",
    "        title_tag = soup.find(['h1', 'h2'], class_=lambda x: x and 'title' in x.lower() if x else False)\n",
    "        if not title_tag:\n",
    "            title_tag = soup.find(['h1', 'h2'])\n",
    "        details['Title'] = title_tag.get_text().strip() if title_tag else \"No title found\"\n",
    "        \n",
    "        # Extract product description\n",
    "        desc_tag = soup.find(['div', 'p'], class_=lambda x: x and 'description' in x.lower() if x else False)\n",
    "        details['Description'] = desc_tag.get_text().strip() if desc_tag else \"No description found\"\n",
    "        \n",
    "        # Extract specs/features\n",
    "        specs = {}\n",
    "        spec_container = soup.find('table') or soup.find('div', class_=lambda x: x and ('spec' in x.lower() or 'detail' in x.lower() or 'feature' in x.lower()) if x else False)\n",
    "        \n",
    "        if spec_container:\n",
    "            rows = spec_container.find_all(['tr', 'li'])\n",
    "            for row in rows:\n",
    "                if row.name == 'tr':\n",
    "                    cols = row.find_all(['th', 'td'])\n",
    "                    if len(cols) >= 2:\n",
    "                        key = cols[0].get_text().strip()\n",
    "                        value = cols[1].get_text().strip()\n",
    "                        specs[key] = value\n",
    "                else:  # li\n",
    "                    text = row.get_text().strip()\n",
    "                    if ':' in text:\n",
    "                        key, value = text.split(':', 1)\n",
    "                        specs[key.strip()] = value.strip()\n",
    "                    else:\n",
    "                        specs[f\"Feature {len(specs) + 1}\"] = text\n",
    "                        \n",
    "        details['Specifications'] = specs\n",
    "        \n",
    "        # Extract images\n",
    "        img_tags = soup.find_all('img', src=True)\n",
    "        product_images = [urljoin(base_url, img['src']) for img in img_tags if 'product' in img.get('class', [''])[0].lower() if img.get('class') else False]\n",
    "        details['Images'] = product_images if product_images else []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting product details: {e}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "# Example usage to get detailed information for a single product\n",
    "# (Uncomment and run after getting the product links)\n",
    "\n",
    "# if not df.empty and 'Link' in df.columns:\n",
    "#     sample_product = df.iloc[0]\n",
    "#     print(f\"Getting details for: {sample_product['Name']}\")\n",
    "#     details = get_product_details(sample_product['Link'])\n",
    "#     print(\"Product Details:\")\n",
    "#     for key, value in details.items():\n",
    "#         print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f99aa",
   "metadata": {},
   "source": [
    "## Alternative Approaches for Data Collection\n",
    "\n",
    "If direct web scraping doesn't work due to website protections, consider these alternatives:\n",
    "\n",
    "### 1. Check for an Official API\n",
    "Many e-commerce platforms provide official APIs for developers to access their data. This is the most proper and reliable way to collect data.\n",
    "\n",
    "### 2. Use Specialized Web Scraping Services\n",
    "Services like:\n",
    "- ScraperAPI\n",
    "- Bright Data (formerly Luminati)\n",
    "- Scraping Bee\n",
    "- Zenscrape\n",
    "\n",
    "These services handle proxy rotation, CAPTCHA solving, and other anti-scraping measures.\n",
    "\n",
    "### 3. Browser Automation with Selenium\n",
    "The Selenium approach in the previous cell can be enhanced with:\n",
    "- Random delays between actions\n",
    "- Mouse movements\n",
    "- Scrolling behaviors\n",
    "\n",
    "### 4. Respect Robots.txt and Website Terms of Service\n",
    "Always check if scraping is allowed by reading the website's:\n",
    "- Robots.txt file\n",
    "- Terms of Service\n",
    "- Fair use policies\n",
    "\n",
    "### 5. Manual Data Collection\n",
    "If the dataset is small, consider manual collection or creating a simple data entry form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f1dc1",
   "metadata": {},
   "source": [
    "## Using ScrapingBee for Web Scraping\n",
    "\n",
    "This notebook now integrates with [ScrapingBee](https://www.scrapingbee.com/), a web scraping API that helps bypass anti-scraping measures. Here's how it works:\n",
    "\n",
    "### Benefits of using ScrapingBee:\n",
    "\n",
    "1. **Proxy Rotation**: ScrapingBee automatically rotates IP addresses to avoid being blocked.\n",
    "2. **JavaScript Rendering**: Renders JavaScript-heavy websites that require a real browser.\n",
    "3. **CAPTCHA Handling**: Helps bypass CAPTCHAs that might be triggered by scraping attempts.\n",
    "4. **Geolocation**: Allows accessing websites as if from specific countries (e.g., Kenya for local websites).\n",
    "5. **Custom Headers & Cookies**: Manages browser fingerprinting to appear as a legitimate user.\n",
    "\n",
    "### Important steps for using ScrapingBee:\n",
    "\n",
    "1. Sign up for an account at [ScrapingBee.com](https://www.scrapingbee.com/)\n",
    "2. Get your API key from the dashboard\n",
    "3. Replace `YOUR_API_KEY` in the code with your actual API key\n",
    "4. Monitor your API usage in the ScrapingBee dashboard\n",
    "\n",
    "### Pricing Considerations:\n",
    "\n",
    "ScrapingBee operates on a credit-based system. Each request costs a certain number of credits depending on features used:\n",
    "- Basic requests: 1 credit\n",
    "- JavaScript rendering: 5 credits\n",
    "- Premium proxies: Additional credits\n",
    "\n",
    "For occasional scraping of a few pages, the free plan (1,000 credits) may be sufficient. For regular scraping, you'll need a paid plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScrapingBee API Usage Tracker\n",
    "def estimate_scrapingbee_usage(pages_scraped, products_per_page=20, detailed_views=0):\n",
    "    \"\"\"\n",
    "    Estimates ScrapingBee API usage and costs\n",
    "    \n",
    "    Args:\n",
    "        pages_scraped: Number of search results pages scraped\n",
    "        products_per_page: Average number of products per page\n",
    "        detailed_views: Number of individual product pages scraped\n",
    "    \"\"\"\n",
    "    # Credits per request with JavaScript rendering + premium proxies\n",
    "    credits_per_request = 5 + 1  # 5 credits for JS rendering + 1 for premium proxies\n",
    "    \n",
    "    # Calculate total requests and credits\n",
    "    search_page_requests = pages_scraped\n",
    "    product_page_requests = detailed_views\n",
    "    total_requests = search_page_requests + product_page_requests\n",
    "    total_credits = total_requests * credits_per_request\n",
    "    \n",
    "    # Print usage summary\n",
    "    print(\"\\n===== ScrapingBee API Usage Estimate =====\")\n",
    "    print(f\"Search pages scraped: {pages_scraped}\")\n",
    "    print(f\"Individual product pages scraped: {detailed_views}\")\n",
    "    print(f\"Total API requests: {total_requests}\")\n",
    "    print(f\"Estimated credits used: {total_credits}\")\n",
    "    \n",
    "    # Show free tier vs paid tier information\n",
    "    print(\"\\n----- Plan Information -----\")\n",
    "    print(\"Free plan: 1,000 credits\")  \n",
    "    print(\"Starter plan: 25,000 credits ($49/month)\")\n",
    "    \n",
    "    # Show credits remaining on free plan\n",
    "    free_tier_remaining = 1000 - total_credits\n",
    "    if free_tier_remaining > 0:\n",
    "        print(f\"\\nCredits remaining on free plan: {free_tier_remaining}\")\n",
    "        print(f\"Can still scrape approximately {free_tier_remaining // credits_per_request} more pages\")\n",
    "    else:\n",
    "        print(f\"\\nFree tier credits exceeded by {abs(free_tier_remaining)}\")\n",
    "        print(f\"Consider upgrading to a paid plan\")\n",
    "\n",
    "# Example usage - uncomment to run after scraping\n",
    "# estimate_scrapingbee_usage(\n",
    "#     pages_scraped=3,  \n",
    "#     products_per_page=20,  \n",
    "#     detailed_views=5  # If you viewed 5 individual product pages\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
